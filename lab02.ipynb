{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:25:37.413777Z",
     "start_time": "2023-11-11T07:25:36.145561Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) \n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    \n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def load_extra_datasets():  \n",
    "    N = 200\n",
    "    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n",
    "    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n",
    "    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n",
    "    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n",
    "    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n",
    "    \n",
    "    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:25:37.436789Z",
     "start_time": "2023-11-11T07:25:37.417233Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer_sizes_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(5, 3)\n",
    "    Y_assess = np.random.randn(2, 3)\n",
    "    return X_assess, Y_assess\n",
    "\n",
    "def initialize_parameters_test_case():\n",
    "    n_x, n_h, n_y = 2, 4, 1\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "def forward_propagation_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "\n",
    "    return X_assess, parameters\n",
    "\n",
    "def compute_cost_test_case():\n",
    "    np.random.seed(1)\n",
    "    Y_assess = np.random.randn(1, 3)\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "\n",
    "    a2 = (np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]))\n",
    "    \n",
    "    return a2, Y_assess, parameters\n",
    "\n",
    "def backward_propagation_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    Y_assess = np.random.randn(1, 3)\n",
    "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
    "        [-0.02136196,  0.01640271],\n",
    "        [-0.01793436, -0.00841747],\n",
    "        [ 0.00502881, -0.01245288]]),\n",
    "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
    "     'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    "     'b2': np.array([[ 0.]])}\n",
    "\n",
    "    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
    "         [-0.05225116,  0.02725659, -0.02646251],\n",
    "         [-0.02009721,  0.0036869 ,  0.02883756],\n",
    "         [ 0.02152675, -0.01385234,  0.02599885]]),\n",
    "  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n",
    "  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n",
    "         [-0.05229879,  0.02726335, -0.02646869],\n",
    "         [-0.02009991,  0.00368692,  0.02884556],\n",
    "         [ 0.02153007, -0.01385322,  0.02600471]]),\n",
    "  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n",
    "    return parameters, cache, X_assess, Y_assess\n",
    "\n",
    "def update_parameters_test_case():\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    " 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    " 'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    " 'b2': np.array([[  9.14954378e-05]])}\n",
    "\n",
    "    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n",
    "        [ 0.00082222, -0.00700776],\n",
    "        [-0.00031831,  0.0028636 ],\n",
    "        [-0.00092857,  0.00809933]]),\n",
    " 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n",
    "          -2.55715317e-03]]),\n",
    " 'db1': np.array([[  1.05570087e-07],\n",
    "        [ -3.81814487e-06],\n",
    "        [ -1.90155145e-07],\n",
    "        [  5.46467802e-07]]),\n",
    " 'db2': np.array([[ -1.08923140e-05]])}\n",
    "    return parameters, grads\n",
    "\n",
    "def nn_model_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    Y_assess = np.random.randn(1, 3)\n",
    "    return X_assess, Y_assess\n",
    "\n",
    "def predict_test_case():\n",
    "    np.random.seed(1)\n",
    "    X_assess = np.random.randn(2, 3)\n",
    "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
    "        [-0.02311792,  0.03137121],\n",
    "        [-0.0169217 , -0.01752545],\n",
    "        [ 0.00935436, -0.05018221]]),\n",
    "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
    "     'b1': np.array([[ -8.97523455e-07],\n",
    "        [  8.15562092e-06],\n",
    "        [  6.04810633e-07],\n",
    "        [ -2.54560700e-06]]),\n",
    "     'b2': np.array([[  9.14954378e-05]])}\n",
    "    return parameters, X_assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Плоская классификация данных с одним скрытым слоем\n",
    "\n",
    "В этой лабораторной требуется построить свою нейросеть с одним скрытым слоем и оценить разницу между этой моделью и той, которая была реализована в прошлой работе.\n",
    "\n",
    "**Вы узнаете как:**\n",
    "- Реализовать нейронную сеть с одним скрытым слоем для бинарной классификации.\n",
    "- Использовать различные нелинейные функции активации, например tanh\n",
    "- Вычислять перекрестную энтропии (cross entropy loss)\n",
    "- Реализовать прямое и обратное распространение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Импорт ##\n",
    "\n",
    "Сначала импортируем все пакеты, которые вам понадобятся во время этого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:25:40.286575Z",
     "start_time": "2023-11-11T07:25:40.267077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # задано значение seed, чтобы результаты были воспроизводимы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Работа с датасетом ##\n",
    "\n",
    "Во-первых, давайте получим набор данных, с которым будем работать. Следующий код загрузит набор данных в переменные `X` и `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:27:01.831311Z",
     "start_time": "2023-11-11T07:27:01.826228Z"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируйте набор данных с помощью matplotlib. Данные выглядят как «цветок» с красными (метка y=0) и синими (y=1) точками. Цель — построить модель, различающую эти классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:32.454865Z",
     "start_time": "2023-11-11T07:53:32.287859Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Визуализируем данные:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется:\n",
    "- numpy-array (матрица) X, содержащая ваши функции (x1, x2)\n",
    "- numpy-array (вектор) Y, содержащий ваши метки (красный: 0, синий: 1).\n",
    "\n",
    "Давайте сначала поймем, что представляют собой наши данные.\n",
    "\n",
    "**Упражнение**. Сколько обучающих примеров у вас есть? Какова размерность переменных «X» и «Y»?\n",
    "\n",
    "**Подсказка**: как получить размерность numpy-array? [(документация)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:30:26.384021Z",
     "start_time": "2023-11-11T07:30:26.378737Z"
    }
   },
   "outputs": [],
   "source": [
    "### Начало кода ### (≈ 3 строки)\n",
    "shape_X = ...\n",
    "shape_Y = ...\n",
    "m = ...  # размер обучающей выборки\n",
    "### конец кода ###\n",
    "\n",
    "print ('Размерность X: ' + str(shape_X))\n",
    "print ('Размерность Y: ' + str(shape_Y))\n",
    "print ('Всего m = %d тренировочных примеров!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "       \n",
    "<table style=\"width:40%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **размерность X** </td>\n",
    "    <td> (2, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**размерность Y**</td>\n",
    "    <td>(1, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>**m**</td>\n",
    "    <td> 400 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Логистическая регрессия\n",
    "\n",
    "Прежде чем строить полную нейронную сеть, давайте сначала посмотрим, как логистическая регрессия справляется с этой задачей. Для этого вы можете использовать встроенные функции sklearn. Запустите приведенный ниже код, чтобы обучить классификатор логистической регрессии на наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:30:53.495615Z",
     "start_time": "2023-11-11T07:30:53.418485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Обучаем классификатор логистической регрессии\n",
    "clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "clf.fit(X.T, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы можете построить график с отображением того, как модель делит данные на классы. Запустите код ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:31:56.610920Z",
     "start_time": "2023-11-11T07:31:56.275884Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# строим график\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n",
    "# Выводим accuracy\n",
    "LR_predictions = clf.predict(X.T)\n",
    "print ('Точность логистической регрессии: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
    "       '% ' + \"(процент правильно размеченных точек данных)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 47% </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерпретация**: Набор данных не является линейно разделимым, поэтому логистическая регрессия работает не очень хорошо. Понадеемся, что нейронная сеть справится лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Нейросетевая модель\n",
    "\n",
    "Логистическая регрессия не справилась с данным датасетом. Обучим нейронную сеть с одним скрытым слоем.\n",
    "\n",
    "**Вот наша модель**:\n",
    "<img src=\"classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Мат.обоснование**:\n",
    "\n",
    "Для одного элемента $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Учитывая предсказания для всех элементов, вы также можете вычислить стоимость $J$ следующим образом:: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Напоминание**: Общая методология построения нейронной сети заключается в следующем:\n",
    " 1. Опредяеем структуру нейронной сети (количество входных блоков, количество скрытых блоков и т. д.). \n",
    " 2. Инициализируем параметры модели\n",
    " 3. В цикле:\n",
    "        - Реализуем прямое распространение\n",
    "        - Считаем потери\n",
    "        - Реализуем обратное распространение, чтобы получить градиенты\n",
    "        - Обновляем параметры (градиентный спуск)\n",
    "\n",
    "Для вычисления шагов 1-3 можно создать вспомогательные функции, а затем объединить их в одну функцию, которую мы называем nn_model(). После построения `nn_model()` и подбора правильных параметров, можно делать прогнозы на основе новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Определяем структуру нейросети ####\n",
    "\n",
    "**Задание**: Инициализируйте три переменные:\n",
    "- n_x: размер входного слоя\n",
    "- n_h: размер скрытого слоя (задайте 4) \n",
    "- n_y: размер выходного слоя\n",
    "\n",
    "**Подсказка**: Используйте размерность X и Y, чтобы найти n_x и n_y. Кроме того, захардкодьте размер скрытого слоя равным 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:35:19.771794Z",
     "start_time": "2023-11-11T07:35:19.768572Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Аргументы:\n",
    "    X -- входной датасет размерностью (input size, количество элементов)\n",
    "    Y -- labels размерностью (output size, количество элементов)\n",
    "    \n",
    "    Возвращает:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \"\"\"\n",
    "    ### Начало кода ### (≈ 3 строчки)\n",
    "    n_x = ... # размер входного слоя\n",
    "    n_h = ...\n",
    "    n_y = ... # размер выходного слоя\n",
    "    ### Конец кода\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:35:21.080510Z",
     "start_time": "2023-11-11T07:35:21.076900Z"
    }
   },
   "outputs": [],
   "source": [
    "X_assess, Y_assess = layer_sizes_test_case()\n",
    "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
    "print(\"Размер входного слоя: n_x = \" + str(n_x))\n",
    "print(\"Размер скрытого слоя: n_h = \" + str(n_h))\n",
    "print(\"Размер выходного слоя: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат** (это не те размеры, которые вы будете использовать для своей сети, они используются просто для оценки функции, которую вы только что закодили)\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "    \n",
    "  <tr>\n",
    "    <td>**n_x**</td>\n",
    "    <td> 5 </td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>**n_h**</td>\n",
    "    <td> 4 </td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>**n_y**</td>\n",
    "    <td> 2 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:35:32.160284Z",
     "start_time": "2023-11-11T07:35:32.157179Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_assess.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 -Иницилиазируем параметры модели ####\n",
    "\n",
    "**Задание**: Реализовать функцию `initialize_parameters()`.\n",
    "\n",
    "**Инструкции**:\n",
    "- Убедитесь, что размерности параметров верны. При необходимости обратитесь к рисунку нейронной сети выше.\n",
    "- Инициализируйте матрицы весов случайными значениями.\n",
    "     - Используйте: `np.random.randn(a,b) * 0,01` для случайной инициализации матрицы формы (a,b).\n",
    "- Инициализируйте свободные члены (bias vectors) нулями.\n",
    "     - Используйте: `np.zeros((a,b))` для инициализации матрицы формы (a,b) нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:36:07.159564Z",
     "start_time": "2023-11-11T07:36:07.094393Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Аргументы:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \n",
    "    Возвращает:\n",
    "    params -- словарь, содержащий ваши параметры:\n",
    "                    W1 -- матрицу весов размерностью (n_h, n_x)\n",
    "                    b1 -- свободные члены (bias vector) размерностью (n_h, 1)\n",
    "                    W2 -- матрицу весов размерностью (n_y, n_h)\n",
    "                    b2 -- свободные члены (bias vector) размерностью (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # мы настраиваем начальное число так, чтобы ваш результат соответствовал нашему, хотя инициализация является случайной.\n",
    "    \n",
    "    ### Начало кода ### (≈ 4 строчки)\n",
    "    W1 = ...\n",
    "    b1 = ...\n",
    "    W2 = ...\n",
    "    b2 = ...\n",
    "    ### Конец кода ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:36:08.410751Z",
     "start_time": "2023-11-11T07:36:08.406036Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = initialize_parameters_test_case()\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00416758 -0.00056267]\n",
    " [-0.02136196  0.01640271]\n",
    " [-0.01793436 -0.00841747]\n",
    " [ 0.00502881 -0.01245288]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Цикл ####\n",
    "\n",
    "**Задание**: Реализуйте `forward_propagation()`.\n",
    "\n",
    "**Инструкции**:\n",
    "- Посмотрите выше на математическое представление модели.\n",
    "- Вы можете использовать функцию `sigmoid()`. Она описана во вспомогательных функциях.\n",
    "- Вы можете использовать функцию `np.tanh()`. Это часть библиотеки numpy.\n",
    "- Шаги, которые необходимо реализовать:\n",
    "    1. Получение каждого параметра из словаря \"parameters\" (который является результатом `initialize_parameters()`), используя `parameters[..]`.\n",
    "    2. Реализовать прямое распространение. Вычислите $Z^{[1]}, A^{[1]}, Z^{[2]}$ и $A^{[2]}$ .\n",
    "- Значения, необходимые для обратного распространения ошибки, хранятся в \"`cache`\".  `cache` будет передан в качестве входных данных для функции обратного распространения ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:37:55.135134Z",
     "start_time": "2023-11-11T07:37:55.128090Z"
    }
   },
   "outputs": [],
   "source": [
    "# функция forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Аргументы:\n",
    "    X -- входные данные размерностью (n_x, m)\n",
    "    parameters -- словарь, содержащий ваши параметры (результат функции инициализации)\n",
    "    \n",
    "    Возвращает:\n",
    "    A2 -- Выход второго слоя\n",
    "    cache -- словарь содержащий \"Z1\", \"A1\", \"Z2\" и \"A2\"\n",
    "    \"\"\"\n",
    "    # Получить каждый параметр из словаря parameters\n",
    "    ### Начало кода ### (≈ 4 строчки)\n",
    "    W1 = ...\n",
    "    b1 = ...\n",
    "    W2 = ...\n",
    "    b2 = ...\n",
    "    ### Конец кода ###\n",
    "    \n",
    "    # Реализуйте прямое распространение, чтобы вычислить A2 (вероятности)\n",
    "    ### Начало кода ### (≈ 4 строчки)\n",
    "    Z1 = ...\n",
    "    A1 = ...\n",
    "    Z2 = ...\n",
    "    A2 = ...\n",
    "    ### Конец кода ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:37:56.250890Z",
     "start_time": "2023-11-11T07:37:56.246471Z"
    }
   },
   "outputs": [],
   "source": [
    "X_assess, parameters = forward_propagation_test_case()\n",
    "\n",
    "A2, cache = forward_propagation(X_assess, parameters)\n",
    "\n",
    "# Примечание. Здесь мы используем среднее значение, чтобы убедиться, что результат соответствует ожидаемому. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "<table style=\"width:55%\">\n",
    "  <tr>\n",
    "    <td> -0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы вычислили $A^{[2]}$ (в нашем случае переменную \"`A2`\"), которая содержит $a^{[2](i)}$ для каждого элемента, можно вычислить функцию стоимости, как показано ниже:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "**Задание**: Реализовать `compute_cost()`, чтобы вычислить значение функции стоимости $J$.\n",
    "\n",
    "**Инструкции**:\n",
    "- Существует много способов рассчитать перекрестную энтропию (cross-entropy loss). Например, так:\n",
    "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs)                # нет необходимости в цикле for!\n",
    "```\n",
    "\n",
    "(можно использовать или `np.multiply()` и затем `np.sum()`, или сразу `np.dot()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:39:40.889012Z",
     "start_time": "2023-11-11T07:39:40.883491Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Вычисляет суммарную перекрестную энтропию (cross-entropy cost), заданную в уравнении (13)\n",
    "    \n",
    "    Аргументы:\n",
    "    A2 -- Выход второго слоя, размерностью (1, количество элементов)\n",
    "    Y -- \"true\" labels вектор размерностью (1, количество экземпляров)\n",
    "    parameters -- словарь, содержащий параметры W1, b1, W2 и b2\n",
    "    \n",
    "    Возвращает:\n",
    "    cost -- cross-entropy cost, данная в уравнении (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # количество элементов\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    ### Начало кода ### (≈ 2 строчки)\n",
    "    logprobs = ...\n",
    "    cost = ...\n",
    "    ### Конец кода ###\n",
    "    \n",
    "    cost = np.squeeze(cost)     # гарантирует, что стоимость соответствует ожидаемому размеру. \n",
    "                                # Например, превращает [[17]] в 17. \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:39:41.106817Z",
     "start_time": "2023-11-11T07:39:41.103092Z"
    }
   },
   "outputs": [],
   "source": [
    "A2, Y_assess, parameters = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**cost**</td>\n",
    "    <td> 0.692919893776 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя `cache`, вычисленный во время прямого распространения, теперь можно реализовать обратное распространение.\n",
    "\n",
    "**Задание**: реализовать функцию `backward_propagation()`.\n",
    "\n",
    "**Инструкции**:\n",
    "Обратное распространение ошибки обычно является самой сложной (наиболее математической) частью глубокого обучения. В помощь слайд из лекции Эндрю Ына. Необходимо использовать шесть уравнений в правой части этого слайда, поскольку мы создаем векторизованную реализацию.  \n",
    "\n",
    "<img src=\"grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "<!--\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
    "\n",
    "- Заметьте, что  $*$ обозначает поэлементное умножение.\n",
    "- Обозначения, которые вы будете использовать, распространены в кодировании глубокого обучения:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "    \n",
    "!-->\n",
    "\n",
    "- Совет:\n",
    "    - Чтобы посчитать dZ1 вам необходимо будет вычислить $g^{[1]'}(Z^{[1]})$. Т.к. $g^{[1]}(.)$ это функция активации tanh, если $a = g^{[1]}(z)$ тогда $g^{[1]'}(z) = 1-a^2$. Поэтому можно вычислить $g^{[1]'}(Z^{[1]})$ используя `(1 - np.power(A1, 2))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:42:39.739562Z",
     "start_time": "2023-11-11T07:42:39.734611Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Реализует обратное распространение, используя инструкции выше.\n",
    "    \n",
    "    Аргументы:\n",
    "    parameters -- словарь, содержащий наши параметры \n",
    "    cache -- словарь содержащий \"Z1\", \"A1\", \"Z2\" и \"A2\".\n",
    "    X -- входные данные размерностью (2, количество элементов)\n",
    "    Y -- \"true\" labels vector размерностью (1, количество элементов)\n",
    "    \n",
    "    Возвращает:\n",
    "    grads -- словарь, содержащий ваши градиенты с оглядкой на разные параметры\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Сначала извлеките W1 и W2 из словаря parameters.\n",
    "    ### начало кода ### (≈ 2 строчки)\n",
    "    W1 = ...\n",
    "    W2 = ...\n",
    "    ### конец кода ###\n",
    "        \n",
    "    # Извлеките A1 и A2 из словаря cache.\n",
    "    ### начало кода ### (≈ 2 строчки)\n",
    "    A1 = ...\n",
    "    A2 = ...\n",
    "    ### конец кода ###\n",
    "    \n",
    "    # Обратное распространение: вычислите dW1, db1, dW2, db2. \n",
    "    ### начало кода ### (≈ 6 строчек, в соответствии с 6 уравнениями на картинке сверху)\n",
    "    dZ2= ...\n",
    "    dW2 = ...\n",
    "    db2 = ...\n",
    "    dZ1 = ...\n",
    "    dW1 = ...\n",
    "    db1 = ...\n",
    "    ### конец кода ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:42:40.230468Z",
     "start_time": "2023-11-11T07:42:40.223079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
    "print(parameters['W1'].shape)\n",
    "print(parameters['W2'].shape)\n",
    "print(X_assess.shape)\n",
    "print(Y_assess.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:42:40.647437Z",
     "start_time": "2023-11-11T07:42:40.640156Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
    "\n",
    "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>**dW1**</td>\n",
    "    <td> [[ 0.01018708 -0.00708701]\n",
    " [ 0.00873447 -0.0060768 ]\n",
    " [-0.00530847  0.00369379]\n",
    " [-0.02206365  0.01535126]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**db1**</td>\n",
    "    <td>  [[-0.00069728]\n",
    " [-0.00060606]\n",
    " [ 0.000364  ]\n",
    " [ 0.00151207]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**dW2**</td>\n",
    "    <td> [[ 0.00363613  0.03153604  0.01162914 -0.01318316]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**db2**</td>\n",
    "    <td> [[ 0.06589489]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**: Реализовать правило обновления с использованием градиентного спуска. Необходимо использовать (dW1, db1, dW2, db2) для обновления (W1, b1, W2, b2).\n",
    "\n",
    "**Общее правило градиентного спуска**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ где $\\alpha$ — это скорость обучения, а $\\theta$ представляет собой параметр.\n",
    "\n",
    "**Иллюстрация**: Алгоритм градиентного спуска с хорошей скоростью обучения (схождение) и плохой скоростью обучения (расхождение).\n",
    "\n",
    "<img src=\"sgd.gif\" style=\"width:400;height:400;\"> <img src=\"sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:44:24.445844Z",
     "start_time": "2023-11-11T07:44:24.441213Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Обновляет параметры, используя правило обновления градиентного спуска, указанное выше.\n",
    "    \n",
    "    Аргументы:\n",
    "    parameters -- словарь, содержащий параметры \n",
    "    grads -- словарь, содержащий градиенты \n",
    "    \n",
    "    Возвращает:\n",
    "    parameters -- словарь, содержащий обновленные параметры \n",
    "    \"\"\"\n",
    "    # Получите каждый параметр из словаря parameters.\n",
    "    ### начало кода ### (≈ 4 строчки)\n",
    "    W1 = ...\n",
    "    b1 = ...\n",
    "    W2 = ...\n",
    "    b2 = ...\n",
    "    ### конец кода ###\n",
    "    \n",
    "    # Получите каждый градиент из словаря grads.\n",
    "    ### начало кода ### (≈ 4 строчки)\n",
    "    dW1 = ...\n",
    "    db1 = ...\n",
    "    dW2 = ...\n",
    "    db2 = ...\n",
    "    ## конец кода ###\n",
    "    \n",
    "    # Обновите значения для каждого параментра\n",
    "    ### начало кода ### (≈ 4 строчки)\n",
    "    W1 = ...\n",
    "    b1 = ...\n",
    "    W2 = ...\n",
    "    b2 = ...\n",
    "    ### конец кода ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:44:24.721507Z",
     "start_time": "2023-11-11T07:44:24.716276Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00643025  0.01936718]\n",
    " [-0.02410458  0.03978052]\n",
    " [-0.01653973 -0.02096177]\n",
    " [ 0.01046864 -0.05990141]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ -1.02420756e-06]\n",
    " [  1.27373948e-05]\n",
    " [  8.32996807e-07]\n",
    " [ -3.20136836e-06]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.00010457]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Объедините части 4.1, 4.2 и 4.3 в nn_model() ####\n",
    "\n",
    "**Задача**: Построить нейросеть в функции `nn_model()`.\n",
    "\n",
    "**Instructions**: Модель нейронной сети должна использовать предыдущие функции в правильном порядке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:45:06.081977Z",
     "start_time": "2023-11-11T07:45:06.076750Z"
    }
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Аргументы:\n",
    "    X -- датасет размерностью (2, количество элементов)\n",
    "    Y -- labels размерностью (1, количество элементов)\n",
    "    n_h -- размер скрытого слоя\n",
    "    num_iterations -- количество итераций в цикле градиентого спуска\n",
    "    print_cost -- если True, выводит cost каждые 1000 итераций\n",
    "    \n",
    "    Возвращает:\n",
    "    parameters -- параметры, изученные моделью. Затем их можно использовать для прогнозирования.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Инициализируйте параметры, затем извлеките W1, b1, W2, b2. Входные данные: «n_x, n_h, n_y». Выходы = «W1, b1, W2, b2, параметры».\n",
    "    ### начало кода ### (≈ 5 строчек)\n",
    "    n_x, n_h, n_y = ...\n",
    "    parameters = ...\n",
    "    W1 = ...\n",
    "    b1 = ...\n",
    "    W2 = ...\n",
    "    b2 = ...\n",
    "    ### конец кода ###\n",
    "    \n",
    "    # Цикл (градиентный спуск)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### начало кода ### (≈ 4 строчки)\n",
    "        # Прямое распространение. Входные параметры: \"X, parameters\". Выходные параметры: \"A2, cache\".\n",
    "        A2, cache = ...\n",
    "        \n",
    "        # Функция стоимости. Входные параметры: \"A2, Y, parameters\". Выходные параметры: \"cost\".\n",
    "        cost = ...\n",
    " \n",
    "        # Обратное распространение. Входные параметры: \"parameters, cache, X, Y\". Выходные параметры: \"grads\".\n",
    "        grads = ...\n",
    " \n",
    "        # Обновление параметра градиентного спуска. Входные параметры: \"parameters, grads\". Выходные параметры: \"parameters\".\n",
    "        parameters = ...\n",
    "        \n",
    "        ### конец кода ###\n",
    "        \n",
    "        # Выводит cost каждые 1000 итераций\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Стоимость после итераций %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:45:07.727019Z",
     "start_time": "2023-11-11T07:45:06.895706Z"
    }
   },
   "outputs": [],
   "source": [
    "X_assess, Y_assess = nn_model_test_case()\n",
    "\n",
    "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-4.18494056  5.33220609]\n",
    " [-7.52989382  1.24306181]\n",
    " [-4.1929459   5.32632331]\n",
    " [ 7.52983719 -1.24309422]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 2.32926819]\n",
    " [ 3.79458998]\n",
    " [ 2.33002577]\n",
    " [-3.79468846]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[-52.66607724]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Предсказания\n",
    "\n",
    "**Задача**: Используйте модель для прогнозирования, для этого напишите функцию predict().\n",
    "Используйте прямое распространение для прогнозирования результатов.\n",
    "\n",
    "**Напоминание**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "Например, если вы хотите установить элементы матрицы X на 0 и 1 на основе порогового значения, вы должны сделать: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:45:52.271461Z",
     "start_time": "2023-11-11T07:45:52.268160Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Используя изученные параметры, прогнозирует класс для каждого примера в X\n",
    "    \n",
    "    Аргументы:\n",
    "    parameters -- словарь, содержащий ваши параметры \n",
    "    X -- входные данные размера (n_x, m)\n",
    "    \n",
    "    Возвращает:\n",
    "    predictions -- Вектор предсказаний нашей модели (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Вычисляет вероятности, используя прямое распространение, и классифицирует их как 0/1, используя 0,5 в качестве порога.\n",
    "    ### начало кода ### (≈ 2 строчки)\n",
    "    A2, cache = ...\n",
    "    predictions = ...\n",
    "    ### конец кода ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:45:52.924892Z",
     "start_time": "2023-11-11T07:45:52.920619Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[0,2,1,3,5,7,6,5]])\n",
    "b = np.array([1  if x>3 else 0  for x  in a.reshape(-1,1)])\n",
    "b = b.reshape(a.shape)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:45:53.788907Z",
     "start_time": "2023-11-11T07:45:53.785462Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters, X_assess = predict_test_case()\n",
    "\n",
    "predictions = predict(parameters, X_assess)\n",
    "print(\"Среднее предсказаний = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**predictions mean**</td>\n",
    "    <td> 0.666666666667 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пришло время запустить модель и посмотреть, как она работает на плоском наборе данных. Запустите следующий код, чтобы протестировать свою модель с одним скрытым слоем из $n_h$ скрытых модулей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:46:00.912541Z",
     "start_time": "2023-11-11T07:45:57.440319Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Создайте модель с n_h-мерным скрытым слоем.\n",
    "parameters = ...\n",
    "\n",
    "# Строим график\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Граница решения для скрытого слоя размера\" + str(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат (округленный до трех знаков)**:\n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**Cost after iteration 9000**</td>\n",
    "    <td> 0.218 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:46:32.822900Z",
     "start_time": "2023-11-11T07:46:32.818823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Выводим accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 90% </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность действительно высока по сравнению с логистической регрессией. Модель выучила узор! Нейронные сети способны изучать даже сильно нелинейные границы разделения классов, в отличие от логистической регрессии.\n",
    "\n",
    "Теперь давайте попробуем несколько размеров скрытого слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Настройка размера скрытого слоя (необязательное/неоцениваемое упражнение) ###\n",
    "\n",
    "Запустите следующий код. Это может занять 1-2 минуты. Вы увидите различное поведение модели для разных размеров скрытых слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:47:32.161518Z",
     "start_time": "2023-11-11T07:47:14.660902Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Это может занять около 2 минут.\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Скрытый слой размера %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    print (\"Точность для {} скрытых нейронов: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерпретация**:\n",
    "- Более крупные модели (с большим количеством скрытых нейронов) могут лучше соответствовать обучающей выборке, пока в конечном итоге самые большие модели не переобучатся под исходные данные.\n",
    "- Лучший размер скрытого слоя, по-видимому, составляет около n_h = 5. Действительно, значение близкое к этому, похоже, хорошо соответствует данным, не вызывая при этом заметного переобучения.\n",
    "- Позже вы также узнаете о регуляризации, которая позволяет использовать очень большие модели (например, n_h = 50) без особого переобучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
